# CCA175考点笔记

LanceLand <<lupindong@gmail.com>> ,v0.0.1,2019.03.30

[TOC]

# 数据摄取
## 使用Sqoop将数据从MySQL数据库导入HDFS

**语法：**

```shell
$ sqoop import [GENERIC-ARGS] [TOOL-ARGS]
```
**示例：**

```shell
$ sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table orders \
--target-dir "/user/cloudera/problem1/orders";
```

## 使用Sqoop从HDFS导出数据到MySQL数据库

**语法：**

```shell
$ sqoop export [GENERIC-ARGS] [TOOL-ARGS]
```

**示例：**

```shell
$ sqoop export \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table orders \
--columns "order_date,order_status,total_amount,total_orders" \
--export-dir "/user/cloudera/problem1/result4a-csv";
```

## 使用Sqoop在导入期间更改数据的分隔符和文件格式

**语法：**

```shell
$ sqoop import [GENERIC-ARGS] [TOOL-ARGS]
```

**示例：**

```shell
$ sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table orders \
--target-dir "/user/cloudera/problem1/orders" \
--fields-terminated-by'|' \
--compress \
--compression-codec snappy \
--as-avrodatafile;

```

## 将实时和近实时流数据摄取到HDFS中

## 在将数据加载到群集时处理流数据

## 使用Hadoop文件系统命令将数据加载到HDFS中和从HDFS加载数据

# 变换，阶段和存储
## 从HDFS加载RDD数据以用于Spark应用程序

## 使用Spark将RDD的结果写回HDFS
## 以各种文件格式读写文件
## 对数据执行标准提取，转换，加载（ETL）过程

# 数据分析
## 将Metastore表用作Spark应用程序的输入源或输出接收器
## 了解Spark中查询数据集的基础知识

```scala
val sc: SparkContext // An existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// Create the DataFrame
val df = sqlContext.read.json("examples/src/main/resources/people.json")
// You can also manually specify the data source
val df = sqlContext.read.format("json").load("examples/src/main/resources/people.json")

// Show the content of the DataFrame
df.show()

// Print the schema in a tree format
df.printSchema()

// Select only the "name" column
df.select("name").show()

// Select everybody, but increment the age by 1
df.select(df("name"), df("age") + 1).show()

// Select people older than 21
df.filter(df("age") > 21).show()

// Count people by age
df.groupBy("age").count().show()
```



## 使用Spark过滤数据
## 编写计算汇总统计信息的查询
## 使用Spark加入不同的数据集
## 生成排名或排序的数据

# 配置
## 提供命令行选项以更改应用程序配置，例如增加可用内存